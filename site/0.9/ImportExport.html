<!DOCTYPE html>
<!--
 | Generated by Apache Maven Doxia at 2016-02-09
 | Rendered using Apache Maven Fluido Skin 1.3.0
-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="Date-Revision-yyyymmdd" content="20160209" />
    <meta http-equiv="Content-Language" content="en" />
    <title>Falcon - Falcon Data Import and Export</title>
    <link rel="stylesheet" href="./css/apache-maven-fluido-1.3.0.min.css" />
    <link rel="stylesheet" href="./css/site.css" />
    <link rel="stylesheet" href="./css/print.css" media="print" />

      
    <script type="text/javascript" src="./js/apache-maven-fluido-1.3.0.min.js"></script>

                          
        
<script type="text/javascript">$( document ).ready( function() { $( '.carousel' ).carousel( { interval: 3500 } ) } );</script>
          
            </head>
        <body class="topBarDisabled">
          
                        
                    
    
        <div class="container">
          <div id="banner">
        <div class="pull-left">
                                <div id="bannerLeft">
                                                                                                <img src="images/falcon-logo.png"  alt="Apache Falcon" width="200px" height="45px"/>
                </div>
                      </div>
        <div class="pull-right">  </div>
        <div class="clear"><hr/></div>
      </div>

      <div id="breadcrumbs">
        <ul class="breadcrumb">
                
                    
                              <li class="">
                    <a href="index.html" title="Falcon">
        Falcon</a>
        </li>
      <li class="divider ">/</li>
        <li class="">Falcon Data Import and Export</li>
        
                
                    
                  <li id="publishDate" class="pull-right">Last Published: 2016-02-09</li> <li class="divider pull-right">|</li>
              <li id="projectVersion" class="pull-right">Version: 0.9</li>
            
                            </ul>
      </div>

      
                
        <div id="bodyColumn" >
                                  
            <div class="section">
<h2>Falcon Data Import and Export<a name="Falcon_Data_Import_and_Export"></a></h2></div>
<div class="section">
<h3>Overview<a name="Overview"></a></h3>
<p>Falcon provides constructs to periodically bring raw data from external data sources (like databases, drop boxes etc) onto Hadoop and push derived data computed on Hadoop onto external data sources.</p>
<p>As of this release, Falcon only supports Relational Databases (e.g. Oracle, MySQL etc) via JDBC as external data source. The future releases will add support for other external data sources.</p></div>
<div class="section">
<h3>Prerequisites<a name="Prerequisites"></a></h3>
<p>Following are the prerequisites to import external data from and export to databases.</p>
<p></p>
<ul>
<li><b>Sqoop 1.4.6+</b></li>
<li><b>Oozie 4.2.0+</b></li>
<li><b>Appropriate database connector</b></li></ul>
<p><b>Note:</b> Falcon uses Sqoop for import/export operation. Sqoop will require appropriate database driver to connect to the relational database. Please refer to the Sqoop documentation for any Sqoop related question. Please make sure the database driver jar is copied into oozie share lib for Sqoop.</p>
<div class="source">
<pre>
For example, in order to import and export with MySQL, please make sure the latest MySQL connector
*mysql-connector-java-5.1.31.jar+* is copied into oozie's Sqoop share lib

/user/oozie/share/lib/{lib-dir}/sqoop/mysql-connector-java-5.1.31.jar+

where {lib-dir} value varies in oozie deployments.


</pre></div></div>
<div class="section">
<h3>Usage<a name="Usage"></a></h3></div>
<div class="section">
<h4>Entity Definition and Setup<a name="Entity_Definition_and_Setup"></a></h4>
<p></p>
<ul>
<li><b>Datasource Entity</b></li></ul>Datasource entity abstracts connection and credential details to external data sources. The Datasource entity       supports read and write interfaces with specific credentials. The default credential will be used if the read       or write interface does not have its own credentials. In general, the Datasource entity will be defined by       system administrator. Please refer to datasource XSD for more details.
<p>The following example defines a Datasource entity for a MySQL database. The import operation will use       the read interface with url &quot;jdbc:mysql://dbhost/test&quot;, user name &quot;import_usr&quot; and password text &quot;sqoop&quot;.       Where as, the export operation will use the write interface with url &quot;jdbc:mysql://dbhost/test&quot; with user       name &quot;export_usr&quot; and password specified in a HDFS file at the location &quot;/user/ambari-qa/password-store/password_write_user&quot;.</p>
<p>The default credential specifies the password using password text and will be used if either read or write interface       does not provide credentials.</p>
<p>The available read and write interfaces enable database administrators to segregate read and write workloads.</p>
<div class="source">
<pre>

      File: mysql-database.xml

      &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
      &lt;datasource colo=&quot;west-coast&quot; description=&quot;MySQL database on west coast&quot; type=&quot;mysql&quot; name=&quot;mysql-db&quot; xmlns=&quot;uri:falcon:datasource:0.1&quot;&gt;
          &lt;tags&gt;owner=foobar@ambari.apache.org, consumer=phoe@ambari.apache.org&lt;/tags&gt;
          &lt;interfaces&gt;
              &lt;!-- ***** read interface ***** --&gt;
              &lt;interface type=&quot;readonly&quot; endpoint=&quot;jdbc:mysql://dbhost/test&quot;&gt;
                  &lt;credential type=&quot;password-text&quot;&gt;
                      &lt;userName&gt;import_usr&lt;/userName&gt;
                      &lt;passwordText&gt;sqoop&lt;/passwordFile&gt;
                  &lt;/credential&gt;
              &lt;/interface&gt;

              &lt;!-- ***** write interface ***** --&gt;
              &lt;interface type=&quot;write&quot;  endpoint=&quot;jdbc:mysql://dbhost/test&quot;&gt;
                  &lt;credential type=&quot;password-file&quot;&gt;
                      &lt;userName&gt;export_usr&lt;/userName&gt;
                      &lt;passwordFile&gt;/user/ambari-qa/password-store/password_write_user&lt;/passwordFile&gt;
                  &lt;/credential&gt;
              &lt;/interface&gt;

              &lt;!-- *** default credential *** --&gt;
              &lt;credential type=&quot;password-text&quot;&gt;
                &lt;userName&gt;sqoop2_user&lt;/userName&gt;
                &lt;passwordText&gt;sqoop&lt;/passwordText&gt;
              &lt;/credential&gt;

          &lt;/interfaces&gt;

          &lt;driver&gt;
              &lt;clazz&gt;com.mysql.jdbc.Driver&lt;/clazz&gt;
              &lt;jar&gt;/user/oozie/share/lib/lib_20150721010816/sqoop/mysql-connector-java-5.1.31&lt;/jar&gt;
          &lt;/driver&gt;
      &lt;/datasource&gt;
      
</pre></div>
<p></p>
<ul>
<li><b>Feed  Entity</b></li></ul>Feed entity now enables users to define IMPORT and EXPORT policies in addition to RETENTION and REPLICATION.       The IMPORT and EXPORT policies will refer to a already defined Datasource entity for connection and credential       details and take a table name from the policy to operate on. Please refer to feed entity XSD for details.
<p>The following example defines a Feed entity with IMPORT and EXPORT policies. Both the IMPORT and EXPORT operations       refer to a datasource entity &quot;mysql-db&quot;. The IMPORT operation will use the read interface and credentials while       the EXPORT operation will use the write interface and credentials. A feed instance is created every 1 hour       since the frequency of the Feed is hour(1) and the Feed instances are deleted after 90 days because of the       retention policy.</p>
<div class="source">
<pre>

      File: customer_email_feed.xml

      &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
      &lt;!--
       A feed representing Hourly customer email data retained for 90 days
       --&gt;
      &lt;feed description=&quot;Raw customer email feed&quot; name=&quot;customer_feed&quot; xmlns=&quot;uri:falcon:feed:0.1&quot;&gt;
          &lt;tags&gt;externalSystem=USWestEmailServers,classification=secure&lt;/tags&gt;
          &lt;groups&gt;DataImportPipeline&lt;/groups&gt;
          &lt;frequency&gt;hours(1)&lt;/frequency&gt;
          &lt;late-arrival cut-off=&quot;hours(4)&quot;/&gt;
          &lt;clusters&gt;
              &lt;cluster name=&quot;primaryCluster&quot; type=&quot;source&quot;&gt;
                  &lt;validity start=&quot;2015-12-15T00:00Z&quot; end=&quot;2016-03-31T00:00Z&quot;/&gt;
                  &lt;retention limit=&quot;days(90)&quot; action=&quot;delete&quot;/&gt;
                  &lt;import&gt;
                      &lt;source name=&quot;mysql-db&quot; tableName=&quot;simple&quot;&gt;
                          &lt;extract type=&quot;full&quot;&gt;
                              &lt;mergepolicy&gt;snapshot&lt;/mergepolicy&gt;
                          &lt;/extract&gt;
                          &lt;fields&gt;
                              &lt;includes&gt;
                                  &lt;field&gt;id&lt;/field&gt;
                                  &lt;field&gt;name&lt;/field&gt;
                              &lt;/includes&gt;
                          &lt;/fields&gt;
                      &lt;/source&gt;
                      &lt;arguments&gt;
                          &lt;argument name=&quot;--split-by&quot; value=&quot;id&quot;/&gt;
                          &lt;argument name=&quot;--num-mappers&quot; value=&quot;2&quot;/&gt;
                      &lt;/arguments&gt;
                  &lt;/import&gt;
                  &lt;export&gt;
                        &lt;target name=&quot;mysql-db&quot; tableName=&quot;simple_export&quot;&gt;
                            &lt;load type=&quot;insert&quot;/&gt;
                            &lt;fields&gt;
                              &lt;includes&gt;
                                &lt;field&gt;id&lt;/field&gt;
                                &lt;field&gt;name&lt;/field&gt;
                              &lt;/includes&gt;
                            &lt;/fields&gt;
                        &lt;/target&gt;
                        &lt;arguments&gt;
                             &lt;argument name=&quot;--update-key&quot; value=&quot;id&quot;/&gt;
                        &lt;/arguments&gt;
                    &lt;/export&gt;
              &lt;/cluster&gt;
          &lt;/clusters&gt;

          &lt;locations&gt;
              &lt;location type=&quot;data&quot; path=&quot;/user/ambari-qa/falcon/demo/primary/importfeed/${YEAR}-${MONTH}-${DAY}-${HOUR}-${MINUTE}&quot;/&gt;
              &lt;location type=&quot;stats&quot; path=&quot;/none&quot;/&gt;
              &lt;location type=&quot;meta&quot; path=&quot;/none&quot;/&gt;
          &lt;/locations&gt;

          &lt;ACL owner=&quot;ambari-qa&quot; group=&quot;users&quot; permission=&quot;0755&quot;/&gt;
          &lt;schema location=&quot;/none&quot; provider=&quot;none&quot;/&gt;

      &lt;/feed&gt;
      
</pre></div>
<p></p>
<ul>
<li><b>Import policy</b></li></ul>The import policy uses the datasource entity specified in the &quot;source&quot; to connect to the database. The tableName      specified should exist in the source datasource.
<p>Extraction type specifies whether to pull data from external datasource &quot;full&quot; everytime or &quot;incrementally&quot;.      The mergepolicy specifies how to organize (snapshot or append, i.e time series partiitons) the data on hadoop.       The valid combinations are:</p>
<ul>
<li>
<ul>
<li>[full,snapshot] - data is extracted in full and dumped into the feed instance location.</li>
<li>[incremental, append] - data is extracted incrementally using the key specified in the <b>deltacolumn</b></li></ul></li></ul>and added as a partition to the feed instance location.
<ul>
<li>
<ul>
<li>[incremental, snapshot] - data is extracted incrementally and merged with already existing data on hadoop to</li></ul></li></ul>produce one latest feed instance.*This feature is not supported currently*. The use case for this feature is          to efficiently import very large dimention tables that have updates and inserts onto hadoop and make it available         as a snapshot with latest updates to consumers.
<p>The following example defines an incremental extraction with append organization:</p>
<div class="source">
<pre>
           &lt;import&gt; 
                &lt;source name=&quot;mysql-db&quot; tableName=&quot;simple&quot;&gt;
                    &lt;extract type=&quot;incremental&quot;&gt;
                        &lt;deltacolumn&gt;modified_time&lt;/deltacolumn&gt;
                        &lt;mergepolicy&gt;append&lt;/mergepolicy&gt;
                    &lt;/extract&gt;  
                    &lt;fields&gt;
                        &lt;includes&gt;
                            &lt;field&gt;id&lt;/field&gt;
                            &lt;field&gt;name&lt;/field&gt;
                        &lt;/includes&gt;
                    &lt;/fields&gt;
                &lt;/source&gt;
                &lt;arguments&gt;
                    &lt;argument name=&quot;--split-by&quot; value=&quot;id&quot;/&gt;
                    &lt;argument name=&quot;--num-mappers&quot; value=&quot;2&quot;/&gt;
                &lt;/arguments&gt;
            &lt;/import&gt;
	
</pre></div>
<p>The fields option enables to control what fields get imported. By default, all fields get import. The &quot;includes&quot; option      brings only those fields specified. The &quot;excludes&quot; option brings all the fields other than specified.</p>
<p>The arguments section enables to pass in any extra arguments needed for fine control on the underlying implementation --      in this case, Sqoop.</p>
<p></p>
<ul>
<li><b>Export policy</b></li></ul>
<p>The export, like import, uses the datasource for connecting to the database. Load type specifies whether to insert      or only update data onto the external table. Fields option behaves the same way as in import policy.      The tableName specified should exist in the external datasource.</p></div>
<div class="section">
<h4>Operation<a name="Operation"></a></h4>
<p>Once the Datasource and Feed entity with import and export policies are defined, Users can submit and schedule    the Import and Export operations via CLI and REST API as below:</p>
<div class="source">
<pre>

    ## submit the mysql-db datasource defined in the file mysql_datasource.xml
    falcon entity -submit -type datasource -file mysql_datasource.xml

    ## submit the customer_feed specified in the customer_email_feed.xml
    falcon entity -submit -type feed -file customer_email_feed.xml

    ## schedule the customer_feed
    falcon entity -schedule -type feed -name customer_feed

   
</pre></div>
<p>Falcon will create corresponding oozie bundles with coordinator and workflow for import and export operation.</p></div>
                  </div>
          </div>

    <hr/>

    <footer>
            <div class="container">
              <div class="row span12">Copyright &copy;                    2013-2016
                        <a href="http://www.apache.org">Apache Software Foundation</a>.
            All Rights Reserved.      
                    
      </div>

                          
                <p id="poweredBy" class="pull-right">
                          <a href="http://maven.apache.org/" title="Built by Maven" class="poweredBy">
        <img class="builtBy" alt="Built by Maven" src="./images/logos/maven-feather.png" />
      </a>
              </p>
        
                </div>
    </footer>
  </body>
</html>
